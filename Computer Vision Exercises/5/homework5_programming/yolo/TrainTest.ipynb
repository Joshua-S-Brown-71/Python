{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6947b46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images for training\n",
      "Output will be saved to `checkpoints`\n",
      "epoch 0/2, iter 0/13, lr 0.000010, loss 7961807.0000\n",
      "epoch 0/2, iter 1/13, lr 0.000010, loss 9534487.0000\n",
      "epoch 0/2, iter 2/13, lr 0.000010, loss 8072014.0000\n",
      "epoch 0/2, iter 3/13, lr 0.000010, loss 7865175.5000\n",
      "epoch 0/2, iter 4/13, lr 0.000010, loss 7922552.0000\n",
      "epoch 0/2, iter 5/13, lr 0.000010, loss 8149928.0000\n",
      "epoch 0/2, iter 6/13, lr 0.000010, loss 7964957.5000\n",
      "epoch 0/2, iter 7/13, lr 0.000010, loss 7907393.0000\n",
      "epoch 0/2, iter 8/13, lr 0.000010, loss 9373734.0000\n",
      "epoch 0/2, iter 9/13, lr 0.000010, loss 7478009.0000\n",
      "epoch 0/2, iter 10/13, lr 0.000010, loss 7017914.0000\n",
      "epoch 0/2, iter 11/13, lr 0.000010, loss 8313464.5000\n",
      "epoch 0/2, iter 12/13, lr 0.000010, loss 3164800.0000\n",
      "yolo_epoch_1.checkpoint.pth\n",
      "epoch 1/2, iter 0/13, lr 0.000010, loss 7874880.5000\n",
      "epoch 1/2, iter 1/13, lr 0.000010, loss 7306433.5000\n",
      "epoch 1/2, iter 2/13, lr 0.000010, loss 8459700.0000\n",
      "epoch 1/2, iter 3/13, lr 0.000010, loss 8100984.0000\n",
      "epoch 1/2, iter 4/13, lr 0.000010, loss 7749644.0000\n",
      "epoch 1/2, iter 5/13, lr 0.000010, loss 7665666.5000\n",
      "epoch 1/2, iter 6/13, lr 0.000010, loss 7310282.5000\n",
      "epoch 1/2, iter 7/13, lr 0.000010, loss 7100471.0000\n",
      "epoch 1/2, iter 8/13, lr 0.000010, loss 6863834.5000\n",
      "epoch 1/2, iter 9/13, lr 0.000010, loss 6819997.5000\n",
      "epoch 1/2, iter 10/13, lr 0.000010, loss 6219164.5000\n",
      "epoch 1/2, iter 11/13, lr 0.000010, loss 6444658.5000\n",
      "epoch 1/2, iter 12/13, lr 0.000010, loss 3481602.0000\n",
      "yolo_epoch_2.checkpoint.pth\n",
      "yolo_final.checkpoint.pth\n",
      "save training loss plot to train_loss.pdf\n",
      "100 images for validation\n",
      "image 1/100, 98 objects detected\n",
      "image 2/100, 98 objects detected\n",
      "image 3/100, 98 objects detected\n",
      "image 4/100, 98 objects detected\n",
      "image 5/100, 98 objects detected\n",
      "image 6/100, 98 objects detected\n",
      "image 7/100, 98 objects detected\n",
      "image 8/100, 98 objects detected\n",
      "image 9/100, 98 objects detected\n",
      "image 10/100, 98 objects detected\n",
      "image 11/100, 98 objects detected\n",
      "image 12/100, 98 objects detected\n",
      "image 13/100, 98 objects detected\n",
      "image 14/100, 98 objects detected\n",
      "image 15/100, 98 objects detected\n",
      "image 16/100, 98 objects detected\n",
      "image 17/100, 98 objects detected\n",
      "image 18/100, 98 objects detected\n",
      "image 19/100, 98 objects detected\n",
      "image 20/100, 98 objects detected\n",
      "image 21/100, 98 objects detected\n",
      "image 22/100, 98 objects detected\n",
      "image 23/100, 98 objects detected\n",
      "image 24/100, 98 objects detected\n",
      "image 25/100, 98 objects detected\n",
      "image 26/100, 98 objects detected\n",
      "image 27/100, 98 objects detected\n",
      "image 28/100, 98 objects detected\n",
      "image 29/100, 98 objects detected\n",
      "image 30/100, 98 objects detected\n",
      "image 31/100, 98 objects detected\n",
      "image 32/100, 98 objects detected\n",
      "image 33/100, 98 objects detected\n",
      "image 34/100, 98 objects detected\n",
      "image 35/100, 98 objects detected\n",
      "image 36/100, 98 objects detected\n",
      "image 37/100, 98 objects detected\n",
      "image 38/100, 98 objects detected\n",
      "image 39/100, 98 objects detected\n",
      "image 40/100, 98 objects detected\n",
      "image 41/100, 98 objects detected\n",
      "image 42/100, 98 objects detected\n",
      "image 43/100, 98 objects detected\n",
      "image 44/100, 98 objects detected\n",
      "image 45/100, 98 objects detected\n",
      "image 46/100, 98 objects detected\n",
      "image 47/100, 98 objects detected\n",
      "image 48/100, 98 objects detected\n",
      "image 49/100, 98 objects detected\n",
      "image 50/100, 98 objects detected\n",
      "image 51/100, 98 objects detected\n",
      "image 52/100, 98 objects detected\n",
      "image 53/100, 98 objects detected\n",
      "image 54/100, 98 objects detected\n",
      "image 55/100, 98 objects detected\n",
      "image 56/100, 98 objects detected\n",
      "image 57/100, 98 objects detected\n",
      "image 58/100, 98 objects detected\n",
      "image 59/100, 98 objects detected\n",
      "image 60/100, 98 objects detected\n",
      "image 61/100, 98 objects detected\n",
      "image 62/100, 98 objects detected\n",
      "image 63/100, 98 objects detected\n",
      "image 64/100, 98 objects detected\n",
      "image 65/100, 98 objects detected\n",
      "image 66/100, 98 objects detected\n",
      "image 67/100, 98 objects detected\n",
      "image 68/100, 98 objects detected\n",
      "image 69/100, 98 objects detected\n",
      "image 70/100, 98 objects detected\n",
      "image 71/100, 98 objects detected\n",
      "image 72/100, 98 objects detected\n",
      "image 73/100, 98 objects detected\n",
      "image 74/100, 98 objects detected\n",
      "image 75/100, 98 objects detected\n",
      "image 76/100, 98 objects detected\n",
      "image 77/100, 98 objects detected\n",
      "image 78/100, 98 objects detected\n",
      "image 79/100, 98 objects detected\n",
      "image 80/100, 98 objects detected\n",
      "image 81/100, 98 objects detected\n",
      "image 82/100, 98 objects detected\n",
      "image 83/100, 98 objects detected\n",
      "image 84/100, 98 objects detected\n",
      "image 85/100, 98 objects detected\n",
      "image 86/100, 98 objects detected\n",
      "image 87/100, 98 objects detected\n",
      "image 88/100, 98 objects detected\n",
      "image 89/100, 98 objects detected\n",
      "image 90/100, 98 objects detected\n",
      "image 91/100, 98 objects detected\n",
      "image 92/100, 98 objects detected\n",
      "image 93/100, 98 objects detected\n",
      "image 94/100, 98 objects detected\n",
      "image 95/100, 98 objects detected\n",
      "image 96/100, 98 objects detected\n",
      "image 97/100, 98 objects detected\n",
      "image 98/100, 98 objects detected\n",
      "image 99/100, 98 objects detected\n",
      "image 100/100, 98 objects detected\n",
      "Detection AP 0.00794079827667324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CS 4391 Homework 5 Programming\n",
    "Run this script for YOLO training\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os, math\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from data import CrackerBox\n",
    "from model import YOLO\n",
    "from loss import compute_loss\n",
    "\n",
    "\n",
    "# plot losses\n",
    "def plot_losses(losses, filename='train_loss.pdf'):\n",
    "\n",
    "    num_epoches = losses.shape[0]\n",
    "    l = np.mean(losses, axis=1)\n",
    "\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.plot(range(num_epoches), l, marker='o', alpha=0.5, ms=4)\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    loss_xlim = plt.xlim()\n",
    "\n",
    "    plt.gcf().set_size_inches(6, 4)\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    print('save training loss plot to %s' % (filename))\n",
    "    plt.clf()\n",
    "    plt.close() \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # hyper-parameters\n",
    "    # you can tune these for your training\n",
    "    num_epochs = 2\n",
    "    batch_size = 8\n",
    "    learning_rate = 1e-5\n",
    "    num_workers = 1\n",
    "\n",
    "    # dataset\n",
    "    dataset_train = CrackerBox('train')  \n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    epoch_size = len(train_loader)\n",
    "\n",
    "    # network\n",
    "    num_classes = 1\n",
    "    num_boxes = 2\n",
    "    network = YOLO(num_boxes, num_classes)\n",
    "    image_size = network.image_size\n",
    "    grid_size = network.grid_size\n",
    "    network.train()\n",
    "\n",
    "    # Optimizer: Adam\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # create output directory\n",
    "    output_dir = 'checkpoints'\n",
    "    print('Output will be saved to `{:s}`'.format(output_dir))\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # save the losses\n",
    "    losses = np.zeros((num_epochs, epoch_size), dtype=np.float32)\n",
    "    # for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # for each sample\n",
    "        for i, sample in enumerate(train_loader):\n",
    "        \n",
    "            image = sample['image']\n",
    "            gt_box = sample['gt_box']\n",
    "            gt_mask = sample['gt_mask']\n",
    "\n",
    "            # forward pass\n",
    "            # Assuming you have this line in your code\n",
    "            outputs = network(image)\n",
    "            fc_output, pred_box = outputs['fc_output'], outputs['pred_box']\n",
    "\n",
    "            # Use fc_output in the compute_loss function\n",
    "            loss = compute_loss(fc_output, pred_box, gt_box, gt_mask, num_boxes, num_classes, grid_size, image_size)\n",
    "\n",
    "            # optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print('epoch %d/%d, iter %d/%d, lr %.6f, loss %.4f' % (epoch, num_epochs, i, epoch_size, learning_rate, loss))\n",
    "            losses[epoch, i] = loss\n",
    "\n",
    "        \n",
    "        # save checkpoint for every epoch\n",
    "        state = network.state_dict()\n",
    "        filename = 'yolo_epoch_{:d}'.format(epoch+1) + '.checkpoint.pth'\n",
    "        torch.save(state, os.path.join(output_dir, filename))\n",
    "        print(filename)\n",
    "        \n",
    "        \n",
    "    # save the final checkpoint\n",
    "    state = network.state_dict()\n",
    "    filename = 'yolo_final.checkpoint.pth'\n",
    "    torch.save(state, os.path.join(output_dir, filename))\n",
    "    print(filename)\n",
    "\n",
    "    # plot loss\n",
    "    plot_losses(losses)\n",
    "\n",
    "\"\"\"\n",
    "CS 4391 Homework 5 Programming\n",
    "Run this script for YOLO testing\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import os, math\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from data import CrackerBox\n",
    "from model import YOLO\n",
    "from voc_eval import voc_eval\n",
    "\n",
    "\n",
    "# from the network prediction, extract the bounding boxes with confidences larger than threshold\n",
    "# pred_box: (batch_size, num_boxes * 5 + num_classes, 7, 7), predicted bounding boxes from the network (see the forward() function)\n",
    "def extract_detections(pred_box, threshold, num_boxes):\n",
    "    # Assuming pred_box is a tensor with shape (batch_size, num_boxes * 5 + num_classes, 7, 7)\n",
    "    \n",
    "    # extract boxes\n",
    "    boxes_all = torch.zeros((0, 5), dtype=torch.float32, device=pred_box.device)\n",
    "    for i in range(num_boxes):\n",
    "        confidence = pred_box[0, 5 * i + 4]\n",
    "        y, x = torch.where(confidence > threshold)\n",
    "        boxes = pred_box[0, 5 * i:5 * i + 5, y, x].t()\n",
    "        boxes_all = torch.cat((boxes_all, boxes), dim=0)\n",
    "\n",
    "    # convert to (x1, y1, x2, y2)\n",
    "    boxes = boxes_all.clone()\n",
    "    boxes[:, 0] = boxes_all[:, 0] - boxes_all[:, 2] * 0.5\n",
    "    boxes[:, 2] = boxes_all[:, 0] + boxes_all[:, 2] * 0.5\n",
    "    boxes[:, 1] = boxes_all[:, 1] - boxes_all[:, 3] * 0.5\n",
    "    boxes[:, 3] = boxes_all[:, 1] + boxes_all[:, 3] * 0.5\n",
    "    return boxes\n",
    "\n",
    "\n",
    "\n",
    "# visualize the detections\n",
    "def visualize(image, gt, detections):\n",
    "    '''\n",
    "    im = image[0].permute(1, 2, 0).cpu().detach().numpy()\n",
    "    pixel_mean = np.array([[[102.9801, 115.9465, 122.7717]]], dtype=np.float32)\n",
    "\n",
    "    # show ground truth\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    im = im * 255.0 + pixel_mean\n",
    "    im = im.astype(np.uint8)\n",
    "    plt.imshow(im[:, :, (2, 1, 0)])\n",
    "    rect = patches.Rectangle((gt[0, 0], gt[0, 1]), gt[0, 2]-gt[0, 0], gt[0, 3]-gt[0, 1], linewidth=2, edgecolor='g', facecolor=\"none\")\n",
    "    ax.add_patch(rect) \n",
    "    plt.title('ground truth')   \n",
    "    \n",
    "    # show detection\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(im[:, :, (2, 1, 0)])\n",
    "    plt.title('prediction')\n",
    "    for i in range(detections.shape[0]):   \n",
    "        x1 = detections[i, 0].detach().numpy()\n",
    "        x2 = detections[i, 2].detach().numpy()\n",
    "        y1 = detections[i, 1].detach().numpy()\n",
    "        y2 = detections[i, 3].detach().numpy()\n",
    "        score = detections[i, 4].detach().numpy()\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='g', facecolor=\"none\")\n",
    "        ax.add_patch(rect)\n",
    "        plt.plot((x1+x2)/2, (y1+y2)/2, 'ro')\n",
    "        ax.text(x1, y1, '%.2f' % score, color='y')\n",
    "    plt.show()'''\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# main function for testing\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # dataset\n",
    "    dataset = CrackerBox('val')  \n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "    epoch_size = len(data_loader)\n",
    "\n",
    "    # network\n",
    "    num_classes = 1\n",
    "    num_boxes = 2\n",
    "    network = YOLO(num_boxes, num_classes)\n",
    "    image_size = network.image_size\n",
    "    grid_size = network.grid_size\n",
    "\n",
    "    # load checkpoint\n",
    "    output_dir = 'checkpoints'\n",
    "    filename = 'yolo_final.checkpoint.pth'\n",
    "    filename = os.path.join(output_dir, filename)\n",
    "    network.load_state_dict(torch.load(filename))\n",
    "    network.eval()\n",
    "    \n",
    "    # detection threshold\n",
    "    threshold = 0.1\n",
    "\n",
    "    # main test loop\n",
    "    results_gt = []\n",
    "    results_pred = []\n",
    "    for i, sample in enumerate(data_loader):\n",
    "\n",
    "        image = sample['image']\n",
    "        gt_box = sample['gt_box']\n",
    "        gt_mask = sample['gt_mask']\n",
    "\n",
    "        # forward pass\n",
    "        outputs = network(image)\n",
    "        output, pred_box = outputs['fc_output'], outputs['pred_box']\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # convert gt box\n",
    "        gt_box = sample['gt_box'][0].numpy()\n",
    "        gt_mask = sample['gt_mask'][0].numpy()\n",
    "        y, x = np.where(gt_mask == 1)\n",
    "        cx = gt_box[0, y, x] * dataset.yolo_grid_size + x * dataset.yolo_grid_size\n",
    "        cy = gt_box[1, y, x] * dataset.yolo_grid_size + y * dataset.yolo_grid_size\n",
    "        w = gt_box[2, y, x] * dataset.yolo_image_size\n",
    "        h = gt_box[3, y, x] * dataset.yolo_image_size\n",
    "        x1 = cx - w * 0.5\n",
    "        x2 = cx + w * 0.5\n",
    "        y1 = cy - h * 0.5\n",
    "        y2 = cy + h * 0.5        \n",
    "        gt = np.array([x1, y1, x2, y2]).reshape((1, 4))\n",
    "        results_gt.append(gt)\n",
    "\n",
    "        # extract predictions\n",
    "        detections = extract_detections(pred_box, threshold, num_boxes)\n",
    "        results_pred.append(detections)\n",
    "        print('image %d/%d, %d objects detected' % (i+1, epoch_size, detections.shape[0]))\n",
    "\n",
    "        # visualization, uncomment the following line to see the detection results\n",
    "        visualize(image, gt, detections)\n",
    "\n",
    "\n",
    "        \n",
    "    # evaluation\n",
    "    rec, prec, ap = voc_eval(results_gt, results_pred)\n",
    "    print('Detection AP', ap)\n",
    "    \n",
    "    # save the PR curve\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    plt.plot(rec, prec)\n",
    "    plt.xlabel('recall')\n",
    "    plt.ylabel('precision')\n",
    "    plt.title('AP: %.2f' % ap)\n",
    "    plt.gcf().set_size_inches(6, 4)\n",
    "    plt.savefig('test_ap.pdf', bbox_inches='tight')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c040d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
