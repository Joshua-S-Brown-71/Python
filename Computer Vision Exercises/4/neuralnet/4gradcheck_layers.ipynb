{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdac2883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running numeric gradient check for fc\n",
      "  grad_x difference:  2.2857848946955528e-10\n",
      "  grad_w difference:  2.5724411489846943e-10\n",
      "  grad_b difference:  1.0486678192478394e-10\n",
      "Running numeric gradient check for relu\n",
      "  grad_x difference:  1.630686696785233e-10\n",
      "Running numeric gradient check for softmax loss\n",
      "  grad_x difference:  0.182651417252296\n",
      "Running numeric gradient check for L2 regularization\n",
      "  grad_w difference:  5.3434465185908664e-11\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils import numeric_backward, numeric_gradient\n",
    "from layers import fc_forward, fc_backward\n",
    "from layers import relu_forward, relu_backward\n",
    "from layers import softmax_loss, l2_regularization\n",
    "\n",
    "\n",
    "def gradcheck_fc():\n",
    "    print('Running numeric gradient check for fc')\n",
    "    N, Din, Dout = 3, 4, 5\n",
    "    x = np.random.randn(N, Din)\n",
    "    w = np.random.randn(Din, Dout)\n",
    "    b = np.random.randn(Dout)\n",
    "\n",
    "    y, cache = fc_forward(x, w, b)\n",
    "    if y is None:\n",
    "        print('  Forward pass is not implemented!')\n",
    "        return\n",
    "\n",
    "    grad_y = np.random.randn(*y.shape)\n",
    "    grad_x, grad_w, grad_b = fc_backward(grad_y, cache)\n",
    "    if grad_x is None or grad_w is None or grad_b is None:\n",
    "        print('  Backward pass is not implemented!')\n",
    "        return\n",
    "\n",
    "    fx = lambda _: fc_forward(_, w, b)[0]\n",
    "    grad_x_numeric = numeric_backward(fx, x, grad_y)\n",
    "    max_diff = np.abs(grad_x - grad_x_numeric).max()\n",
    "    print('  grad_x difference: ', max_diff)\n",
    "\n",
    "    fw = lambda _: fc_forward(x, _, b)[0]\n",
    "    grad_w_numeric = numeric_backward(fw, w, grad_y)\n",
    "    max_diff = np.abs(grad_w - grad_w_numeric).max()\n",
    "    print('  grad_w difference: ', max_diff)\n",
    "\n",
    "    fb = lambda _: fc_forward(x, w, _)[0]\n",
    "    grad_b_numeric = numeric_backward(fb, b, grad_y)\n",
    "    max_diff = np.abs(grad_b - grad_b_numeric).max()\n",
    "    print('  grad_b difference: ', max_diff)\n",
    "\n",
    "\n",
    "def gradcheck_relu():\n",
    "    print('Running numeric gradient check for relu')\n",
    "    N, Din = 4, 5\n",
    "    x = np.random.randn(N, Din)\n",
    "\n",
    "    y, cache = relu_forward(x)\n",
    "    if y is None:\n",
    "        print('  Forward pass is not implemented!')\n",
    "        return\n",
    "\n",
    "    grad_y = np.random.randn(*y.shape)\n",
    "    grad_x = relu_backward(grad_y, cache)\n",
    "    if grad_x is None:\n",
    "        print('  Backward pass is not implemented!')\n",
    "        return\n",
    "\n",
    "    f = lambda _: relu_forward(_)[0]\n",
    "    grad_x_numeric = numeric_backward(f, x, grad_y)\n",
    "    max_diff = np.abs(grad_x - grad_x_numeric).max()\n",
    "    print('  grad_x difference: ', max_diff)\n",
    "\n",
    "\n",
    "def gradcheck_softmax():\n",
    "    print('Running numeric gradient check for softmax loss')\n",
    "    N, C = 4, 5\n",
    "    x = np.random.randn(N, C)\n",
    "    y = np.random.randint(C, size=(N,))\n",
    "    loss, grad_x = softmax_loss(x, y)\n",
    "    if loss is None or grad_x is None:\n",
    "        print('  Softmax not implemented!')\n",
    "        return\n",
    "\n",
    "    f = lambda _: softmax_loss(_, y)[0]\n",
    "    grad_x_numeric = numeric_gradient(f, x)\n",
    "    max_diff = np.abs(grad_x - grad_x_numeric).max()\n",
    "    print('  grad_x difference: ', max_diff)\n",
    "\n",
    "\n",
    "def gradcheck_l2_regularization():\n",
    "    print('Running numeric gradient check for L2 regularization')\n",
    "    Din, Dout = 3, 4\n",
    "    reg = 0.1\n",
    "    w = np.random.randn(Din, Dout)\n",
    "    loss, grad_w = l2_regularization(w, reg)\n",
    "    if loss is None or grad_w is None:\n",
    "        print('  L2 regularization not implemented!')\n",
    "        return\n",
    "\n",
    "    f = lambda _: l2_regularization(_, reg)[0]\n",
    "    grad_w_numeric = numeric_gradient(f, w)\n",
    "    max_diff = np.abs(grad_w - grad_w_numeric).max()\n",
    "    print('  grad_w difference: ', max_diff)\n",
    "\n",
    "\n",
    "def main():\n",
    "    gradcheck_fc()\n",
    "    gradcheck_relu()\n",
    "    gradcheck_softmax()\n",
    "    gradcheck_l2_regularization()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da81d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
